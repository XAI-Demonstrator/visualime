{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ab3149",
   "metadata": {},
   "source": [
    "# VisuaLIME example\n",
    "\n",
    "This brief introduction show you how to generate a visual explanation for the classification of an image by a deep learning computer vision model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4468954",
   "metadata": {},
   "source": [
    "## Load an image-classification model\n",
    "\n",
    "For this example, we're using a relatively small pre-trained model provided as part of the Tensorflow deep learning library.\n",
    "\n",
    "If you haven't installed Tensorflow in your current Python environment, uncomment and run the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056d646",
   "metadata": {},
   "source": [
    "Then, we can load the modell and the corresponding preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17296f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ecd944",
   "metadata": {},
   "source": [
    "Since LIME is a black box explanation method, it does not \"know\" anything about how to call the model to produce predictions. Instead, we need to provide a function that simply takes an array of images and returns the corresponding outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a070ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(image):\n",
    "    return model.predict(preprocess_input(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6d745",
   "metadata": {},
   "source": [
    "## Load an image\n",
    "\n",
    "We'll load an image hosted on the internet as part of the [XAI Demonstrator project](https://github.com/xai-demonstrator/xai-demonstrator). Instead, you can also load an image from your harddrive or from a different URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f37932",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image = Image.open(urlopen(\"https://storage.googleapis.com/xai-demo-assets/visual-inspection/images/table.jpg\"))\n",
    "full_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002884f",
   "metadata": {},
   "source": [
    "We'll just select a single object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = full_image.crop((766, 90, 990, 314))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f169d05",
   "metadata": {},
   "source": [
    "VisuaLIME takes in images as Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(img)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1569a",
   "metadata": {},
   "source": [
    "Note that the image is 224 by 224 pixels, which is exactly the size our `model` expects. In general, it is advisable to compute explanations on the same scale as the model's input. So in case your image is larger or smaller than the size expected by the model, rescale it prior to passing it to the VisuaLIME algorithm.\n",
    "\n",
    "Let's see whether we can predict what's in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_fn(image[None,:,:,:])\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e74f94",
   "metadata": {},
   "source": [
    "We see that the output contains one classification result of length 1000. Each of the 1000 entries corresponds to the likelihood that the image belongs to that particular class.\n",
    "\n",
    "Let's see what the model sees in the picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eda21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def66251",
   "metadata": {},
   "source": [
    "So it's class 759. We can decode this either by looking up the ImageNet categories or using the provided decoder function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import decode_predictions\n",
    "decode_predictions(prediction, top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52a7c6",
   "metadata": {},
   "source": [
    "Great, so the model correctly identifies the camera in the image. But what exactly does it look at?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd0150",
   "metadata": {},
   "source": [
    "## Compute an explanation\n",
    "\n",
    "To find out, import the two main functions of `visualime`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualime.explain import explain_classification, render_explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68463d9f",
   "metadata": {},
   "source": [
    "First, we'll compute the explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_mask, segment_weights = explain_classification(image=image, predict_fn=predict_fn, num_of_samples=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293daefb",
   "metadata": {},
   "source": [
    "Then, we can generate the visual output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_explanation(image, segment_mask, segment_weights, positive=\"green\", negative=\"red\", coverage=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893b270",
   "metadata": {},
   "source": [
    "So it seems like the lens is a strong factor in influencing the model's decision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
